{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45feaa06-246d-4db5-b396-4ae03955cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e834f670-8e0a-42c9-8feb-0fe693496976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6184f154-2ef6-4b70-b7e1-6611c9157be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b68ac8-c953-4616-aea1-27f75e27962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if use_GPU:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print (\"MPS device available.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print (\"MPS device not found. Using CPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print (\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f91d4-59c4-4327-b3d3-eeb2c4f1a95a",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf124b6-cf70-4fbf-aea1-45aaadb412f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the file\n",
    "filepath = '../data/input.txt'\n",
    "if not os.path.exists(filepath):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7cb886-719a-4eea-b5aa-c8bd73c9ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Reading the file\n",
    "with open(filepath) as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dcb2b1-5a22-402d-90b6-0b08a09d10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448f2fc4-c7a1-4907-800a-3ad80ca94483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 65\n",
      "encode.    hello   => [46, 43, 50, 50, 53, 2]\n",
      "decode.    [46, 43, 50, 50, 53, 2]   => hello!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ordered list of all characters in the corpus\n",
    "vocab = sorted(list(set(raw_text)))\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "# dictionaries to convert chars to ints, and viceversa\n",
    "itos = {i:v for i,v in enumerate(vocab)}\n",
    "stoi = {v:i for i,v in enumerate(vocab)}\n",
    "\n",
    "# functions to tokenize sequences of arbitrary length\n",
    "encode = lambda x : [stoi[s] for s in x]\n",
    "decode = lambda x : ''.join([itos[i] for i in x])\n",
    "\n",
    "hello_encoded = encode('hello!')\n",
    "hello_decoded = decode(hello_encoded)\n",
    "print(f'encode.    hello   => {hello_encoded}')\n",
    "print(f'decode.    {hello_encoded}   => {hello_decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdda0100-1254-4c05-831b-cbc55704ef28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing the text an creating a numerical tensor\n",
    "token_text = torch.tensor(encode(raw_text))\n",
    "token_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3660e2-505c-4400-b2b2-be7e7ff3a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# Splitting data in train, val\n",
    "train_size = int(0.9 * len(token_text))\n",
    "data_train = token_text[:train_size]\n",
    "data_val = token_text[train_size:]\n",
    "print(data_train.shape, data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cadc9a-c803-4502-9d70-195be9579e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddddde4b-7ceb-4a1f-b16c-92426f0fdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training batches\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4   # the number of sample processed in the same batch\n",
    "block_size = 8   # the length of the sequence of character used by the model to predict the next character\n",
    "\n",
    "def get_batch(data, batch_size, block_size, device):\n",
    "\n",
    "    # generating the start index of  batch_size independent samples\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,), device=device)\n",
    "\n",
    "    # extracting the xi, each having block_size elements\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    #print(x.device)\n",
    "\n",
    "    # extracting the yi, the next characters\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in idx]) \n",
    "\n",
    "    # moving tensors to device\n",
    "    #x, y = x.to(device), y.to(device)\n",
    "    #x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = get_batch(data_train, batch_size, block_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827aa29d-d78a-453e-b8eb-10ead4940dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b3adf4-c2e5-4c8b-9d87-8aac37f8e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the validation data\n",
    "X_val, y_val = get_batch(data_val, data_val.shape[0], block_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7f158e-8dfb-4ee6-8deb-2d83564a0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train)\n",
    "print()\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f6934-a4aa-42ef-aa38-4095a1f10dc1",
   "metadata": {},
   "source": [
    "# Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c2bc9-e0d6-4ca6-8ce0-1cdef5522800",
   "metadata": {},
   "source": [
    "## Bigram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba49595-2bc7-4d7e-85c3-701e994048e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts next character using only the previous character\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # A table mapping the index of each character, to the logits distribution of the next character\n",
    "        # The element E[i,j] represent the logit associated to next character being j, when the current \n",
    "        # character is i\n",
    "        self.E = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        # extracting the logits of the next character, for each character in the input\n",
    "        # The shape of the input is (batch_size, block_size)\n",
    "        # The shape of the output is (batch_size, block_size, vocab_size)\n",
    "        logits = self.E(X)\n",
    "\n",
    "        # computing loss, only when a target y is provided\n",
    "        if y is not None:\n",
    "            # reshaping logits, and target to match the expected shapes of the cross_entropy function\n",
    "            batch_size, block_size, vocab_size = logits.shape\n",
    "            logits = logits.view((batch_size*block_size, vocab_size))\n",
    "            y = y.view(batch_size*block_size)\n",
    "            \n",
    "            # computing the loss\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, X, max_length=100):\n",
    "\n",
    "        for i in range(max_length):\n",
    "            # performing a forward pass\n",
    "            logits, loss = self.forward(X)\n",
    "    \n",
    "            # condsidering only the last character of each sequence\n",
    "            logits = logits[:, -1, :]\n",
    "    \n",
    "            # computing probs, from the logits\n",
    "            probs = logits.softmax(dim=-1)\n",
    "    \n",
    "            # sampling from the probability distributions\n",
    "            new_element = torch.multinomial(probs, 1)\n",
    "    \n",
    "            # attaching the new elements to the context\n",
    "            X = torch.concat((X, new_element), axis=1)\n",
    "\n",
    "        # Decoding the generated sequences\n",
    "        res = [decode(X[i].tolist()) for i in range(X.shape[0]) ]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e164e577-5b86-403c-ad07-e01e25e6d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.6161, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(x_train, y_train)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "724f4af3-bd59-4db4-9119-faee516b875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i,rrPAUpt$ZA.!v!FwnToT;jo\n",
      "bn-lx:co$SfhlLPXWEuxJXopRjyXIKFlCjFCGgX$'PV-G;VDrMCCsT?WakygEz3kncyDvwhbNr\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from UNTRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, 1)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m.generate(context, max_length=100)\n",
    "\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0b7d6f-0917-4012-bd59-2a0e1785ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "4.9710493087768555\n",
      "2.2749061584472656\n",
      "2.069427251815796\n",
      "2.72166109085083\n",
      "2.6071584224700928\n",
      "2.0997016429901123\n",
      "2.4455082416534424\n",
      "2.4198312759399414\n",
      "2.3851704597473145\n",
      "2.281238317489624\n",
      "\n",
      "VALIDATION\n",
      "tensor(2.5036, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Training a model with Adam Optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=10**(-3))\n",
    "\n",
    "print('TRAINING')\n",
    "steps = 100000\n",
    "for i in range(steps):\n",
    "\n",
    "    # generating a random batch\n",
    "    X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = m(X_b, y_b)\n",
    "\n",
    "    # bacward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # printing at 10% intervals\n",
    "    if i % int(steps / 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print()\n",
    "print('VALIDATION')\n",
    "logits, loss = m(X_val, y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63dbe722-fb69-44c6-98f7-df2df2c44c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whe s nde irand.\n",
      "T:\n",
      "ON 'stavend bed rdowortis s-seravef dwis.\n",
      "tht uthast d, w, is to od, an!\n",
      "\n",
      "WAST: hot chis;\n",
      "RKINouthorerts m murs o s acomyoo fre.\n",
      "Whorfres l weend s and morur bre s s e'thanertrodwha h! as nd d forosthithaly mofrt,\n",
      "\n",
      "\n",
      "VOLOROFawheam or s arintheey\n",
      "\n",
      "JUM:\n",
      "CIfere whetarsuind t hofen f SCHAUSofond y thes we n-\n",
      "\n",
      "Sou Hathe\n",
      "'toulle idat! fif? mert,\n",
      "\n",
      "'sthoo.\n",
      "Sould CLLiss\n",
      "\n",
      "\n",
      "Th!\n",
      "IUCA ty blland bu I'st him.\n",
      "\n",
      "Th bllinsindofr nor fr LUpr beweme\n",
      "ANGRD buret Ne rt k minewarthe.\n",
      "\n",
      "\n",
      "Ayowise t I b\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from TRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, 1)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m.generate(context, max_length=500)\n",
    "\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406f0ee-800e-4ddd-b9e5-f28520bf56aa",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6b23776-3031-4785-9275-06d8ef2fe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "X_i = tensor([[ 3.,  5.],\n",
      "        [ 8., 10.],\n",
      "        [ 6.,  2.]])\n",
      "\n",
      "Z_i = tensor([[ 3.,  5.],\n",
      "        [11., 15.],\n",
      "        [17., 17.]])\n"
     ]
    }
   ],
   "source": [
    "# We want to create a transformation matrix that for each token, collects information for all \n",
    "# the previous tokens in the sequence.\n",
    "# Let's assume that emb_size = 2, and block_size=3\n",
    "# If we have a matrix \n",
    "#     \n",
    "# W = [[ 1, 0, 0]\n",
    "#      [ 1, 1, 0]\n",
    "#      [ 1, 1, 1]]\n",
    "#\n",
    "# and sample X_i representing a sequence of tokens of size emb size\n",
    "#\n",
    "# X_i =[[ 3, 5]       # token at time 0\n",
    "#       [ 8, 10]      # token at time 1\n",
    "#       [ 6, 2]]      # token at time 2\n",
    "#\n",
    "# then if we perform a matrix multiplication between W and  and X_i\n",
    "# we obtain \n",
    "#\n",
    "# Z_i = [[ 3, 5]      # token at time 0\n",
    "#        [11, 15]     # token at time 1\n",
    "#        [17, 17]]    # token at time 2\n",
    "# \n",
    "# where the token at time j is obtained by summing al the tokens until time j included.\n",
    "\n",
    "W = torch.tril(torch.ones((3,3)))\n",
    "print(f'W = {W}')\n",
    "\n",
    "X_i = torch.tensor([[ 3, 5],\n",
    "                    [ 8, 10],\n",
    "                    [ 6, 2]] ).float()\n",
    "print()\n",
    "print(f'X_i = {X_i}')\n",
    "\n",
    "Z_i = W @ X_i\n",
    "print()\n",
    "print(f'Z_i = {Z_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b63784b-a3aa-49f1-ab4d-dc29e294a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "X_i = tensor([[ 3.,  5.],\n",
      "        [ 8., 10.],\n",
      "        [ 6.,  2.]])\n",
      "\n",
      "Z_i = tensor([[3.0000, 5.0000],\n",
      "        [5.5000, 7.5000],\n",
      "        [5.6667, 5.6667]])\n"
     ]
    }
   ],
   "source": [
    "# Since we want merge into each time step the knowledge from the previous time steps\n",
    "# a simple mean would be more appropriate than a sum\n",
    "W = torch.tril(torch.ones((3,3))) \n",
    "W /= W.sum(axis=1, keepdims=True)\n",
    "print(f'W = {W}')\n",
    "\n",
    "X_i = torch.tensor([[ 3, 5],\n",
    "                    [ 8, 10],\n",
    "                    [ 6, 2]] ).float()\n",
    "print()\n",
    "print(f'X_i = {X_i}')\n",
    "\n",
    "Z_i = W @ X_i\n",
    "print()\n",
    "print(f'Z_i = {Z_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccafba02-3ccc-4fb6-b2ce-93531dff7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = tensor([[1.0000,   -inf,   -inf],\n",
      "        [0.5000, 0.5000,   -inf],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "X_i = tensor([[ 3.,  5.],\n",
      "        [ 8., 10.],\n",
      "        [ 6.,  2.]])\n",
      "\n",
      "Z_i = tensor([[3.0000, 5.0000],\n",
      "        [5.5000, 7.5000],\n",
      "        [5.6667, 5.6667]])\n"
     ]
    }
   ],
   "source": [
    "# In practice we want to be able to learn the matrix W by backpropagation, and not set them manually.\n",
    "# If we start from a triangular matrix, and keep updating it through backpropagation, there is no\n",
    "# guarantee that zeros will stay 0.\n",
    "# One trick to keep the triangular shape of the matrix is to replace the zeros with -inf:\n",
    "#   - no amount of change will change the -inf\n",
    "#   - when, eventually, applying a softmax to the result the -inf values will become zeros\n",
    "\n",
    "W = torch.tril(torch.ones((3,3))) \n",
    "W /= W.sum(axis=1, keepdims=True)\n",
    "W[W == 0.] = -torch.inf\n",
    "print(f'W = {W}')\n",
    "\n",
    "X_i = torch.tensor([[ 3, 5],\n",
    "                    [ 8, 10],\n",
    "                    [ 6, 2]] ).float()\n",
    "print()\n",
    "print(f'X_i = {X_i}')\n",
    "\n",
    "Z_i = W.softmax(axis=1) @ X_i\n",
    "print()\n",
    "print(f'Z_i = {Z_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86ee1ef9-fafb-447c-9ce8-aec0082585cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape = torch.Size([40, 10])\n",
      "K.shape = torch.Size([40, 10])\n",
      "W.shape = torch.Size([40, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\na = torch.randn(50,100)\\nprint(a.std(axis= 1).mean())\\nb = torch.randn(50,100)\\nprint(b.std(axis= 1).mean())\\nab = (a @ b.T) \\nprint(ab.std(axis= 1).mean())\\nab_norm = ab * 100**-0.5\\nprint(ab_norm.std(axis= 1).mean())\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In the transformer architecture the matrix W of size (context_size, context_size) is not a raw parameter,\n",
    "# but rather the result of some computations.\n",
    "# The matrix X_e, of size (context_size, emb_size) where each row is a token in the space of size emb_size\n",
    "# is first projected  into a smaller space of size head_size. \n",
    "# This projection is a actually repeated three times to obtain matrices with different meanings:\n",
    "#   - The rows of the matrix K represent the information with which a token should be indexed\n",
    "#   - The rows of the matrix Q represent the information that a token is interested in\n",
    "#   - The rows of the matrix V are projections of X_e that contains the info to be shareds\n",
    "#\n",
    "# When multiplying  matrix K of size (context_size, emb_size) and the transposse of Q\n",
    "# of size (context_size, emb_size) we obtain a matrix of size (context_size, context_size)\n",
    "# where the element at position (i,j) represents the amount of information that needs to be\n",
    "# shared from token at position j to the token at position i.\n",
    "# This matrix has the same shape and function as the matrix W discussed before.\n",
    "# Adding a softmax operation we can obtain a matrix where each row is a probability distribution,\n",
    "# and the element (i,j) gives us exactly the fraction of the token j that we want to transfer \n",
    "# to the token i.\n",
    "#\n",
    "# The last steps to perform is multilplying the self-attention matrix W by the input to operate the transfer \n",
    "# of information among tokens.\n",
    "# This operation is performed in a space of size head_size, using the projected matrix V, \n",
    "# instead of the original matrix X_e.\n",
    "#\n",
    "#  The result of the attention mechanism is thus:\n",
    "#   Z = softmax(Q x K.T) x V\n",
    "#\n",
    "#  The only detail remaining is the the normalization of the (Q x K.T) matrix to insure that the rows have unit variance.\n",
    "#  If it's not the case (Q x K.T) might contain values that have large magnitude, and after applying the softmax\n",
    "#  operation, the result will tend be a one-hot vector where only 1 or a few elemnts are different from zero,\n",
    "#  while the otheres will be virtually zero.\n",
    "#  Under this situation the attention mechanism will be greatly compromised, as a token will receive information\n",
    "#  only from a handful of tokens in the context.\n",
    "# Ex\n",
    "\n",
    "context_size = 40\n",
    "head_size = 10\n",
    "\n",
    "# Generating Q and k\n",
    "Q = torch.randn(context_size, head_size)\n",
    "print(f'Q.shape = {Q.shape}')\n",
    "K = torch.randn(context_size, head_size)\n",
    "print(f'K.shape = {K.shape}')\n",
    "\n",
    "\n",
    "# Multiplying the Q and K to obtain the W matrix\n",
    "W = Q @ K.T\n",
    "print(f'W.shape = {W.shape}')\n",
    "\n",
    "# Normalizing with sqrt(head_size)\n",
    "W_norm  = W / head_size**0.5\n",
    "\n",
    "# Compting softmax of bot original and normalized\n",
    "W_soft = W.softmax(axis=1)\n",
    "W_norm_soft = W_norm.softmax(axis=1)\n",
    "\n",
    "# Comparing the weight distribution with, and without normalization\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax0.set_title('W without norm')\n",
    "ax0.imshow(W_soft)\n",
    "ax1.set_title('W with Norm')\n",
    "ax1.imshow(W_norm_soft)\n",
    "\n",
    "\n",
    "'''\n",
    "a = torch.randn(50,100)\n",
    "print(a.std(axis= 1).mean())\n",
    "b = torch.randn(50,100)\n",
    "print(b.std(axis= 1).mean())\n",
    "ab = (a @ b.T) \n",
    "print(ab.std(axis= 1).mean())\n",
    "ab_norm = ab * 100**-0.5\n",
    "print(ab_norm.std(axis= 1).mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35993ded-d06c-47a5-9df7-7fe302d76ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts next character using all previous characters\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, block_size, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # A table mapping the index of each character, to an embedding vector X_e_0 of size emb_size\n",
    "        self.E = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # # A table mapping the position of each character, to an embedding vector X_e_1 of size emb_size\n",
    "        self.P = nn.Embedding(block_size, emb_size)\n",
    "\n",
    "        # The transformations mapping X_e to a space of size head_size.\n",
    "        # X_e is a \"private\" representation of a token.\n",
    "        # From this vector we can project different  representations\n",
    "        #   - K => represents the information that X_e could provide (KEY)\n",
    "        #   - Q => represents the information that X_e might require (QUERY)\n",
    "        #   - V => represents the information that X_e will actually provide (VALUE)\n",
    "        self.W_k = nn.Linear(emb_size, head_size)\n",
    "        self.W_q = nn.Linear(emb_size, head_size)\n",
    "        self.W_v = nn.Linear(emb_size, head_size)\n",
    "        \n",
    "        # Output layer to produce the logits\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (batch_size, block_size)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e_0 = self.E(X)\n",
    "\n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (block_size)\n",
    "        # The shape of the output is (block_size, emb_size)\n",
    "        X_pos = torch.arange(0, self.block_size)  # creating matrix with positions: [0, 1, ...]\n",
    "        X_e_1 = self.P(X_pos)   \n",
    "\n",
    "        # Adding together the embedding vectors\n",
    "        # The shape of X_e_0 is (batch_size, block_size, emb_size)\n",
    "        # The shape of X_e_1 is  (block_size, emb_size)\n",
    "        # (Broadcasting will make the operation possible)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e = X_e_0 + X_e_1\n",
    "\n",
    "\n",
    "        # Projecting the embedding vector X_e to obtain K, Q, V in the space of size head_size\n",
    "        K = self.W_k(X_e)\n",
    "        Q = self.W_q(X_e)\n",
    "        V = self.W_v(X_e)\n",
    "\n",
    "        # Matrix providing the attention mechanism\n",
    "        # - It is obtained comparing one by one, the rows of K with the rows of Q\n",
    "        # - The scalar product between the i-th row of K and j-th row of Q represents\n",
    "        #   the 'affinity' between the token at position i and the token at position j\n",
    "        # - We divide by sqrt(emb_size) to ensure that the rows have unit variance\n",
    "        # - We mask it with a triangular matrix to avoid contamination from tokens \n",
    "        #   in the future\n",
    "        W  = torch.tril(Q @ K.transpose(1,2))  # we transpose only the last 2 dims. 1st dim is batch \n",
    "        W /= W.sum(axis=1, keepdims=True)\n",
    "        W[W == 0.] = -torch.inf\n",
    "\n",
    "        # Applying attention mechanism to get info from previous steps in the block\n",
    "        # The mechanism is applied to the projection V\n",
    "        X_a = W.softmax(axis=1) @ V\n",
    "        \n",
    "        # Extracting the logits of the next character, for each character in the input\n",
    "        # The shape of the input is (batch_size, block_size, emb_size)\n",
    "        # The shape of the output is (batch_size, block_size, vocab_size)\n",
    "        logits = self.output_layer(X_a)\n",
    "\n",
    "        # computing loss, only when a target y is provided\n",
    "        if y is not None:\n",
    "            # reshaping logits, and target to match the expected shapes of the cross_entropy function\n",
    "            batch_size, block_size, vocab_size = logits.shape\n",
    "            logits = logits.view((batch_size*block_size, vocab_size))\n",
    "            y = y.view(batch_size*block_size)\n",
    "            \n",
    "            # computing the loss\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, X, max_length=100):\n",
    "\n",
    "        # initilizing the result\n",
    "        res = X.clone()\n",
    "\n",
    "        for i in range(max_length):\n",
    "            # performing a forward pass\n",
    "            logits, loss = self.forward(X)\n",
    "    \n",
    "            # condsidering only the last character of each sequence\n",
    "            logits = logits[:, -1, :]\n",
    "    \n",
    "            # computing probs, from the logits\n",
    "            probs = logits.softmax(dim=-1)\n",
    "    \n",
    "            # sampling from the probability distributions\n",
    "            new_element = torch.multinomial(probs, 1)\n",
    "    \n",
    "            # attaching the new elements to the context\n",
    "            # and excluding the oldest element\n",
    "            X = torch.concat((X[:, 1:], new_element), axis=1)\n",
    "\n",
    "            # Storing the generated element\n",
    "            res = torch.concat((res, new_element), axis=1)\n",
    "\n",
    "        # Decoding the generated sequences\n",
    "        res = [decode(res[i].tolist()) for i in range(res.shape[0]) ]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b029c013-ed66-47dc-bce3-67c7aed3e549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.2762, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 32\n",
    "head_size = 16\n",
    "m1 = TransformerLanguageModel(vocab_size, emb_size, block_size, head_size)\n",
    "logits, loss = m1(x_train, y_train)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4a1eac8-4059-4614-ab98-d50f883970cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "4.262965679168701\n",
      "3.232572078704834\n",
      "3.3648524284362793\n",
      "3.1062920093536377\n",
      "3.0491156578063965\n",
      "2.8436172008514404\n",
      "3.102666139602661\n",
      "2.591468572616577\n",
      "2.5304019451141357\n",
      "2.7577946186065674\n",
      "\n",
      "VALIDATION\n",
      "tensor(2.7700, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Training a model with Adam Optimizer\n",
    "optimizer1 = torch.optim.Adam(m1.parameters(), lr=10**(-3))\n",
    "\n",
    "print('TRAINING')\n",
    "steps = 100000\n",
    "for i in range(steps):\n",
    "\n",
    "    # generating a random batch\n",
    "    X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = m1(X_b, y_b)\n",
    "\n",
    "    # bacward pass\n",
    "    optimizer1.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    # printing at 10% intervals\n",
    "    if i % int(steps / 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print()\n",
    "print('VALIDATION')\n",
    "logits, loss = m1(X_val, y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dc43da8-3e0f-4825-89d3-8535363e8f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUENEN:\n",
      "WThe fice tore.\n",
      "\n",
      "NGOLDVO:\n",
      "Mar thisal, lo byoull lillo to fromind acd wilis cous.\n",
      "Whi theras?\n",
      "\n",
      "Sany outournd ticomous cou?\n",
      "\n",
      "ATher, wisto th anoug cere.\n",
      "Bus,\n",
      "H'se fris tyous gme?\n",
      "And sded fusil t these?\n",
      "CATheres:\n",
      "\n",
      "Theeserer'd wis adsthe thereint akd'd ands omon;\n",
      "D Buso mal byes th isad my arde\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from TRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, block_size)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m1.generate(context, max_length=300)\n",
    "\n",
    "print(res[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5960fae-0e31-4b61-82af-dbed35624589",
   "metadata": {},
   "source": [
    "## Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88f306dd-860d-4a2a-919a-20482103052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to represent a single attention head\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The transformations mapping X_e to a space of size head_size.\n",
    "        # X_e is a \"private\" representation of a token.\n",
    "        # From this vector we can project different  representations\n",
    "        #   - K => represents the information that X_e could provide (KEY)\n",
    "        #   - Q => represents the information that X_e might require (QUERY)\n",
    "        #   - V => represents the information that X_e will actually provide (VALUE)\n",
    "        self.W_k = nn.Linear(emb_size, head_size)\n",
    "        self.W_q = nn.Linear(emb_size, head_size)\n",
    "        self.W_v = nn.Linear(emb_size, head_size)\n",
    "\n",
    "    \n",
    "    def forward(self, X_e):\n",
    "\n",
    "        # Getting device where data is residing\n",
    "        device = X_e.device\n",
    "        \n",
    "        # Projecting the embedding vector X_e to obtain K, Q, V in the space of size head_size\n",
    "        K = self.W_k(X_e)\n",
    "        Q = self.W_q(X_e)\n",
    "        V = self.W_v(X_e)\n",
    "\n",
    "        # Matrix providing the attention mechanism\n",
    "        # - It is obtained comparing one by one, the rows of K with the rows of Q\n",
    "        # - The scalar product between the i-th row of K and j-th row of Q represents\n",
    "        #   the 'affinity' between the token at position i and the token at position j\n",
    "        # - We divide by sqrt(emb_size) to ensure that the rows have unit variance\n",
    "        # - We mask it with a triangular matrix to avoid contamination from tokens \n",
    "        #   in the future\n",
    "        W  = torch.tril(Q @ K.transpose(1,2)).to(device)  # we transpose only the last 2 dims. 1st dim is batch \n",
    "        W /= W.sum(axis=1, keepdims=True)\n",
    "        W[W == 0.] = -torch.inf\n",
    "\n",
    "        # Applying attention mechanism to get info from previous steps in the block\n",
    "        # The mechanism is applied to the projection V\n",
    "        X_a = W.softmax(axis=1) @ V\n",
    "        \n",
    "        return X_a\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31d4daf2-e5ee-4fa9-8519-30dbf64bcc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones((3,3))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "049f9996-0a5d-4ac3-8429-5029242c9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah = AttentionHead(30, 10)\n",
    "a = torch.randn((32, 200, 30))\n",
    "ah(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "869ed937-5170-45f1-a1f5-eb3b530671bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to represent multiple attention heads applied in parallel\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads_number, emb_size, head_size, ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializing heads_number attention heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(emb_size, head_size) for i in range(heads_number)])\n",
    "        \n",
    "    \n",
    "    def forward(self, X_e):\n",
    "\n",
    "        # Applying the attention heads separately, and concatenating the results\n",
    "        X_ma = torch.concat([ah(X_e) for ah in self.heads], axis=-1)\n",
    "        \n",
    "        return X_ma\n",
    "\n",
    "    '''\n",
    "    def to(device):\n",
    "        new_self = super(MultiAttentionHead, self).to(device)\n",
    "        new_self.heads = [ah.to(device) for ah in new_self.heads]\n",
    "    \n",
    "        return new_self\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6196319e-0a12-4e1b-bdb4-2169a25e9f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200, 50])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah = MultiAttentionHead(5, 30, 10)\n",
    "a = torch.randn((32, 200, 30))\n",
    "ah(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8edc284b-1e69-4aaf-8067-a9b4d8592a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts next character using all previous characters\n",
    "# Uses multiple attention heads\n",
    "\n",
    "class MultiHeadTransformerLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, block_size, head_size, heads_numbers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # A table mapping the index of each character, to an embedding vector X_e_0 of size emb_size\n",
    "        self.E = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # # A table mapping the position of each character, to an embedding vector X_e_1 of size emb_size\n",
    "        self.P = nn.Embedding(block_size, emb_size)\n",
    "\n",
    "        # The multi-head attention  block\n",
    "        self.MAH = MultiAttentionHead(heads_numbers, emb_size, head_size)\n",
    "        \n",
    "        # Output layer to produce the logits\n",
    "        self.output_layer = nn.Linear(head_size*heads_numbers, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        # Getting device where data is residing\n",
    "        device = X.device\n",
    "        \n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (batch_size, block_size)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e_0 = self.E(X)\n",
    "\n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (block_size)\n",
    "        # The shape of the output is (block_size, emb_size)\n",
    "        X_pos = torch.arange(0, self.block_size, device=device)  # creating matrix with positions: [0, 1, ...]\n",
    "        X_e_1 = self.P(X_pos)   \n",
    "\n",
    "        # Adding together the embedding vectors\n",
    "        # The shape of X_e_0 is (batch_size, block_size, emb_size)\n",
    "        # The shape of X_e_1 is  (block_size, emb_size)\n",
    "        # (Broadcasting will make the operation possible)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e = X_e_0 + X_e_1\n",
    "\n",
    "        # Applying the multiple attention heads \n",
    "        X_a = self.MAH(X_e)\n",
    "        \n",
    "        # Extracting the logits of the next character, for each character in the input\n",
    "        # The shape of the input is (batch_size, block_size, emb_size)\n",
    "        # The shape of the output is (batch_size, block_size, vocab_size)\n",
    "        logits = self.output_layer(X_a)\n",
    "\n",
    "        # computing loss, only when a target y is provided\n",
    "        if y is not None:\n",
    "            # reshaping logits, and target to match the expected shapes of the cross_entropy function\n",
    "            batch_size, block_size, vocab_size = logits.shape\n",
    "            logits = logits.view((batch_size*block_size, vocab_size))\n",
    "            y = y.view(batch_size*block_size)\n",
    "            \n",
    "            # computing the loss\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, X, max_length=100):\n",
    "\n",
    "        # initilizing the result\n",
    "        res = X.clone()\n",
    "\n",
    "        for i in range(max_length):\n",
    "            # performing a forward pass\n",
    "            logits, loss = self.forward(X)\n",
    "    \n",
    "            # condsidering only the last character of each sequence\n",
    "            logits = logits[:, -1, :]\n",
    "    \n",
    "            # computing probs, from the logits\n",
    "            probs = logits.softmax(dim=-1)\n",
    "    \n",
    "            # sampling from the probability distributions\n",
    "            new_element = torch.multinomial(probs, 1)\n",
    "    \n",
    "            # attaching the new elements to the context\n",
    "            # and excluding the oldest element\n",
    "            X = torch.concat((X[:, 1:], new_element), axis=1)\n",
    "\n",
    "            # Storing the generated element\n",
    "            res = torch.concat((res, new_element), axis=1)\n",
    "\n",
    "        # Decoding the generated sequences\n",
    "        res = [decode(res[i].tolist()) for i in range(res.shape[0]) ]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ef71f49-e2f1-4c03-a99f-62f6bdd53bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3e17933-2783-4f18-8738-6e8b359240e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.2218, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#device = torch.device(\"mps\")\n",
    "emb_size = 32\n",
    "head_size = 8\n",
    "head_numbers = 4\n",
    "m2 = MultiHeadTransformerLanguageModel(vocab_size, emb_size, block_size, head_size, head_numbers)\n",
    "m2.to(device)\n",
    "logits, loss = m2(x_train, y_train)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81124442-5204-4182-b00f-40d9fc430b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "4.2646565437316895\n",
      "2.7620913982391357\n",
      "2.6786835193634033\n",
      "2.490812063217163\n",
      "2.955991744995117\n",
      "2.7675678730010986\n",
      "3.134674310684204\n",
      "2.7351832389831543\n",
      "3.260215997695923\n",
      "2.4945969581604004\n",
      "\n",
      "VALIDATION\n",
      "tensor(2.8049, grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 9min 15s, sys: 12min 32s, total: 21min 48s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training a model with Adam Optimizer\n",
    "optimizer2 = torch.optim.Adam(m2.parameters(), lr=10**(-3))\n",
    "\n",
    "print('TRAINING')\n",
    "steps = 100000\n",
    "for i in range(steps):\n",
    "\n",
    "    # generating a random batch\n",
    "    #X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    \n",
    "    # forward pass\n",
    "    logits, loss = m2(X_b, y_b)\n",
    "\n",
    "    # bacward pass\n",
    "    optimizer2.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # printing at 10% intervals\n",
    "    if i % int(steps / 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print()\n",
    "print('VALIDATION')\n",
    "logits, loss = m2(X_val, y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00066c21-a2a4-42ad-ae16-4b463bf715cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUENSES:\n",
      "Torowe ko ndo, wto n the,\n",
      "Wy isfroud;\n",
      "I''s spallll.\n",
      "I l Iwol, y aks corme ascuplo bll, alllll.\n",
      "KI I Od I Iw thals anst.\n",
      "Th alilds halpot,\n",
      "Thes akan hy teras;\n",
      "Tin hart mis.\n",
      "LI'd I this sto thing mit'ss th.\n",
      "An t hadd bire tong ghar?\n",
      "Cisin,\n",
      "Cand alss?\n",
      "NGo forore bus cecore?\n",
      "Mrom lblones,\n",
      "To co\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from TRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, block_size)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m2.generate(context, max_length=300)\n",
    "\n",
    "print(res[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876f191-92cf-43b5-b859-3cd17a0bc7fd",
   "metadata": {},
   "source": [
    "# Multiple decoder layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3872ca36-d2fa-4863-a898-9d2fc2acfd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizing the architecture to have multiple decoder layers.\n",
    "# Each decoder layer is composed of :\n",
    "#   - a multiattention head\n",
    "#   - a fully connected feed forward layer\n",
    "#   - ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd43390c-a90a-4562-ab68-d2165728d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network transforming embed_size feature in new embed_size features.\n",
    "# The main diferrence between the attention block and the feed forward block are that:\n",
    "#   - the attention layer performs data transformation in the time dimension, by looking updating\n",
    "#     each feature of a token, by looking at the values of the same feature for tokens that apperead\n",
    "#     at previous times.\n",
    "#   - the feed forward layer creating new features for a given token by combining all the features of the same token,\n",
    "#     and without using any info from other token.\n",
    "#\n",
    "# The feed forward networks create new embed_size features in the following way:\n",
    "#  - create new K * embed_size features with a MLP\n",
    "#  - projects the K * embed_size into embed_size with a linear transformation\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, K=4):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(emb_size, K * emb_size)\n",
    "        self.R = nn.ReLU()\n",
    "        self.W2 = nn.Linear(K * emb_size, emb_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # applying:\n",
    "        #  - linear transformation\n",
    "        #  - non linearity\n",
    "        #  - linear transformation\n",
    "        return self.W2(self.R(self.W1(X)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fcd96447-83f7-4de2-9a5b-8541221bd4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((32, 20, 10))\n",
    "ff = FeedForward(10)\n",
    "ff(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "051c1896-abf6-41fc-a3cf-de1b1daf5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, heads_numbers, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The multi-head attention  block\n",
    "        self.MAH = MultiAttentionHead(heads_numbers, emb_size, head_size)\n",
    "\n",
    "        # The layer normalization for the outputs of the attention block\n",
    "        self.LN1 = nn.LayerNorm([emb_size])\n",
    "\n",
    "        # The feed forward layer\n",
    "        self.FF = FeedForward(emb_size)\n",
    "\n",
    "        # The layer normalization for the outputs of the attention block\n",
    "        self.LN2 = nn.LayerNorm([emb_size])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Sharing informtion between tokens:\n",
    "        X_a = self.MAH(X)\n",
    "        # Normalizing so that the varince of each token is 1\n",
    "        X_a = self.LN1(X_a)\n",
    "        # Adding skip connection\n",
    "        X_a = X_a + X\n",
    "        \n",
    "        # Sharing information between the components of each token\n",
    "        X_f = self.FF(X_a)\n",
    "        # Normalizing so that the varince of each token is 1\n",
    "        X_f = self.LN2(X_f)\n",
    "        # Adding skip connection\n",
    "        X_f = X_f + X_a\n",
    "            \n",
    "        return X_f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0907795c-0f59-4fb4-95ea-b3255f1fdfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 30])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((32, 20, 30))\n",
    "el = EncoderLayer(3, 30, 10)\n",
    "y = el(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be321e37-603d-4f13-ba76-7535d1714bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3.7305), tensor(3.7253), tensor(3.7247)]\n",
      "[tensor(19.9721), tensor(19.9255), tensor(41.8956)]\n",
      "[tensor(0.4655, grad_fn=<MeanBackward0>), tensor(0.4643, grad_fn=<MeanBackward0>), tensor(1.0102, grad_fn=<MeanBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(13, (32, 20, 30)).float()\n",
    "print([x.std(axis=i).mean() for i in range(3)])\n",
    "W = torch.randn(30, 50)\n",
    "x1 = x @ W\n",
    "print([x1.std(axis=i).mean() for i in range(3)])\n",
    "LN = nn.LayerNorm([50])\n",
    "y = LN(x1)\n",
    "print([y.std(axis=i).mean() for i in range(3)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1caea9e0-b41f-4aa7-95d5-2c94553da2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts next character using all previous characters\n",
    "# Uses multiple encoder layers\n",
    "\n",
    "class MultiEncoderLayersLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, layers_number, vocab_size, emb_size, block_size, head_size, heads_number):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # A table mapping the index of each character, to an embedding vector X_e_0 of size emb_size\n",
    "        self.E = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # A table mapping the position of each character, to an embedding vector X_e_1 of size emb_size\n",
    "        self.P = nn.Embedding(block_size, emb_size)\n",
    "\n",
    "        # The encoder layers\n",
    "        self.EL = nn.Sequential(*[EncoderLayer(heads_number, emb_size, head_size) \n",
    "                                 for i in range(layers_number)])\n",
    "\n",
    "        # The layer normalization to apply before the last layer\n",
    "        self.LN = nn.LayerNorm([head_size*heads_number])\n",
    "        \n",
    "        # Output layer to produce the logits\n",
    "        self.output_layer = nn.Linear(head_size*heads_number, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        # Getting device where data is residing\n",
    "        device = X.device\n",
    "        \n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (batch_size, block_size)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e_0 = self.E(X)\n",
    "\n",
    "        # Transforming the input indices to embedding vectors\n",
    "        # The shape of the input is (block_size)\n",
    "        # The shape of the output is (block_size, emb_size)\n",
    "        X_pos = torch.arange(0, self.block_size, device=device)  # creating matrix with positions: [0, 1, ...]\n",
    "        X_e_1 = self.P(X_pos)   \n",
    "\n",
    "        # Adding together the embedding vectors\n",
    "        # The shape of X_e_0 is (batch_size, block_size, emb_size)\n",
    "        # The shape of X_e_1 is  (block_size, emb_size)\n",
    "        # (Broadcasting will make the operation possible)\n",
    "        # The shape of the output is (batch_size, block_size, emb_size)\n",
    "        X_e = X_e_0 + X_e_1\n",
    "\n",
    "        # Applying the encoder layers\n",
    "        X_a = self.EL(X_e)\n",
    "\n",
    "        # Applying the layer normalization\n",
    "        X_a = self.LN(X_a)\n",
    "        \n",
    "        # Extracting the logits of the next character, for each character in the input\n",
    "        # The shape of the input is (batch_size, block_size, emb_size)\n",
    "        # The shape of the output is (batch_size, block_size, vocab_size)\n",
    "        logits = self.output_layer(X_a)\n",
    "\n",
    "        # computing loss, only when a target y is provided\n",
    "        if y is not None:\n",
    "            # reshaping logits, and target to match the expected shapes of the cross_entropy function\n",
    "            batch_size, block_size, vocab_size = logits.shape\n",
    "            logits = logits.view((batch_size*block_size, vocab_size))\n",
    "            y = y.view(batch_size*block_size)\n",
    "            \n",
    "            # computing the loss\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, X, max_length=100):\n",
    "\n",
    "        # initilizing the result\n",
    "        res = X.clone()\n",
    "\n",
    "        for i in range(max_length):\n",
    "            # performing a forward pass\n",
    "            logits, loss = self.forward(X)\n",
    "    \n",
    "            # condsidering only the last character of each sequence\n",
    "            logits = logits[:, -1, :]\n",
    "    \n",
    "            # computing probs, from the logits\n",
    "            probs = logits.softmax(dim=-1)\n",
    "    \n",
    "            # sampling from the probability distributions\n",
    "            new_element = torch.multinomial(probs, 1)\n",
    "    \n",
    "            # attaching the new elements to the context\n",
    "            # and excluding the oldest element\n",
    "            X = torch.concat((X[:, 1:], new_element), axis=1)\n",
    "\n",
    "            # Storing the generated element\n",
    "            res = torch.concat((res, new_element), axis=1)\n",
    "\n",
    "        # Decoding the generated sequences\n",
    "        res = [decode(res[i].tolist()) for i in range(res.shape[0]) ]\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec10dd-055c-4e36-a3bb-d3355bf30dda",
   "metadata": {},
   "source": [
    "## 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d208733-2344-4f01-95a1-1eaab14e2333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.2525, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 32\n",
    "head_size = 8\n",
    "head_numbers = 4\n",
    "m3 = MultiEncoderLayersLanguageModel(1, vocab_size, emb_size, block_size, head_size, head_numbers)\n",
    "m3.to(device)\n",
    "logits, loss = m3(x_train, y_train)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0abf1ee2-d728-4b7f-9414-1f02903b0ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "4.5573039054870605\n",
      "2.4879300594329834\n",
      "2.2736194133758545\n",
      "1.884597659111023\n",
      "2.193108320236206\n",
      "2.0519917011260986\n",
      "2.158106565475464\n",
      "2.104804277420044\n",
      "1.7611197233200073\n",
      "2.0642142295837402\n",
      "\n",
      "VALIDATION\n",
      "tensor(2.0711, grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 11min 41s, sys: 15min 22s, total: 27min 3s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training a model with Adam Optimizer\n",
    "optimizer3 = torch.optim.Adam(m3.parameters(), lr=10**(-3))\n",
    "\n",
    "print('TRAINING')\n",
    "steps = 100000\n",
    "for i in range(steps):\n",
    "\n",
    "    # generating a random batch\n",
    "    #X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    \n",
    "    # forward pass\n",
    "    logits, loss = m3(X_b, y_b)\n",
    "\n",
    "    # bacward pass\n",
    "    optimizer3.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "\n",
    "    # printing at 10% intervals\n",
    "    if i % int(steps / 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print()\n",
    "print('VALIDATION')\n",
    "logits, loss = m3(X_val, y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d2a5741-41b3-440f-a0f2-6ab1a3709352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCLESLI:\n",
      "Plarst.\n",
      "\n",
      "RICCATHANLANE:\n",
      "Werwill de.\n",
      "\n",
      "CARBUTIO:\n",
      "By O hane,\n",
      "Whe andess call ath pis.\n",
      "\n",
      "My'so Cak o blin fig Reast, fth, o.\n",
      "\n",
      "RIV:\n",
      "By Geart nodll't bot atherle w hard id na f iredecl comehof h fricomblese,\n",
      "That plouring ibesin com jonssecturt be hear?\n",
      "SICELKIABONVELIUCET:\n",
      "Dn py haf f youlcedes;\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from TRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, block_size)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m3.generate(context, max_length=300)\n",
    "\n",
    "print(res[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ee468-bce3-4e51-be84-787f60c9b3f0",
   "metadata": {},
   "source": [
    "## 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa8b8d71-401c-4ba7-8bd2-2f940a1d344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.3400, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 32\n",
    "head_size = 8\n",
    "head_numbers = 4\n",
    "m4 = MultiEncoderLayersLanguageModel(2, vocab_size, emb_size, block_size, head_size, head_numbers)\n",
    "m4.to(device)\n",
    "logits, loss = m4(x_train, y_train)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6f22320-7de1-4cf8-a2e6-b103ccc49830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "4.531033039093018\n",
      "1.9015980958938599\n",
      "2.156529664993286\n",
      "2.5381391048431396\n",
      "1.8989261388778687\n",
      "2.076533079147339\n",
      "1.9867684841156006\n",
      "1.7173433303833008\n",
      "1.571828842163086\n",
      "1.8651556968688965\n",
      "\n",
      "VALIDATION\n",
      "tensor(1.6103, grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 22min 29s, sys: 29min 41s, total: 52min 11s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training a model with Adam Optimizer\n",
    "optimizer4 = torch.optim.Adam(m4.parameters(), lr=10**(-3))\n",
    "\n",
    "print('TRAINING')\n",
    "steps = 100000\n",
    "for i in range(steps):\n",
    "\n",
    "    # generating a random batch\n",
    "    #X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    X_b, y_b = get_batch(data_train, batch_size, block_size, device)\n",
    "    \n",
    "    # forward pass\n",
    "    logits, loss = m4(X_b, y_b)\n",
    "\n",
    "    # bacward pass\n",
    "    optimizer4.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer4.step()\n",
    "\n",
    "    # printing at 10% intervals\n",
    "    if i % int(steps / 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print()\n",
    "print('VALIDATION')\n",
    "logits, loss = m4(X_val, y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60a542a6-c969-41de-bb77-67bec1b4b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COFING VICENSA:\n",
      "God rair.\n",
      "\n",
      "Mut Mord got\n",
      "Bule,\n",
      "VNE I ther, so.\n",
      "\n",
      "Pin it of bease heans\n",
      "Thice our s pyet.\n",
      "\n",
      "Fo wild no?\n",
      "DUSES:\n",
      "Amy stak Momm grong wit astent Wutiiot.\n",
      "\n",
      "JAl, beait I mor los wingy?\n",
      "\n",
      "Roy'l nie.\n",
      "\n",
      "COF IY gral hith whil To now's\n",
      "M:\n",
      "My to dome not, to me. horre?\n",
      "Saur grout Yor A.\n",
      "\n",
      "ESINE:\n",
      "My I\n"
     ]
    }
   ],
   "source": [
    "# Generating sequence from TRAINED model\n",
    "\n",
    "# creating initial context\n",
    "context = torch.zeros((1, block_size)).int()\n",
    "\n",
    "# generating sequence of arbitrary length \n",
    "res = m4.generate(context, max_length=300)\n",
    "\n",
    "print(res[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30144fa1-8f44-40f3-84ef-5233e33b4b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c66dc-3127-46eb-ad1d-23e5ad6b5e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
